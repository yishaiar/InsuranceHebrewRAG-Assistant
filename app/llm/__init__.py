
from ollama import chat,generate,pull
import os

def prompt_llm(prompt,model):
  pull(model)# download the model if not exists using ollama
  stream = chat(
      model=model,
      messages=[{'role': 'user', 'content': prompt}],
      stream=True,
  )
  full_text = ''
  for chunk in stream:
    chunk_text = chunk['message']['content']
    print(chunk_text, end='', flush=True)
    full_text += chunk_text
  return full_text

def format_chunks(chunks):
    formatted_context = "\n".join([f"{i+1}. {chunk.strip()}" for i, chunk in enumerate(chunks)])
    return formatted_context

# Function to load the markdown template and replace placeholders with actual data
def load_and_format_prompt(user_query, retrieved_chunks,prompt_template_path = 'app/llm/prompt_template.md'):
    # format the retrieved chunks
    formatted_chunks = format_chunks(retrieved_chunks)
        
    # Load the markdown template
    with open(prompt_template_path, 'r') as file:
        prompt_template = file.read()

    # Replace placeholders with actual data
    formatted_prompt = prompt_template.replace("{{user_query}}", user_query).replace("{{formatted_chunks}}", formatted_chunks)
    
    return formatted_prompt





def print_prompt_and_response(user_query, retrieved_chunks,model,print_rag_results=False):
    """
    This function prints out the user's input prompt and the final LLM response.
    
    Parameters:
    - user_query (str): The user's input free text query.
    - llm_response (str): The final response generated by the LLM.
    """
    
    print("===== User Input Query =====")
    print(user_query)
    if print_rag_results:
        print("===== Relevant RAG Data =====")
        for chunk in retrieved_chunks:
            print(chunk)
            print('-'*50)
    print("\n===== Final LLM Response =====")
    formatted_prompt = load_and_format_prompt(user_query, retrieved_chunks)
    llm_response = prompt_llm(formatted_prompt,model)
    print('='*150)
    print('='*150)
    return {'user_query':user_query,'retrieved_chunks':retrieved_chunks,'llm_response':llm_response}
    
